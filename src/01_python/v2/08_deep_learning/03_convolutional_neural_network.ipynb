{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3DR-eO17geWu"
      },
      "source": [
        "# Convolutional Neural Network\n",
        "Yann Lecun @ Facebook-> grandfather of CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EMefrVPCg-60"
      },
      "source": [
        "### Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2.15.0'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oxQxCBWyoGPE"
      },
      "source": [
        "## Part 1 - Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MvE-heJNo3GG"
      },
      "source": [
        "### Preprocessing the Training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 8000 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "# 1) transformations are required to avoid overfitting\n",
        "# by modifying images, we get \"image augmentation\" (new images)\n",
        "# 2) images reduced from 150x150 to 64x64 to minimise calc time\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0 / 255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True\n",
        ")\n",
        "\n",
        "# `flow_from_directory` method identifies classes based on the directory structure:\n",
        "# 2 classes because there were /dogs and /cats\n",
        "training_set = train_datagen.flow_from_directory(\n",
        "    \"./filez_cats_dogs/training_set/\",\n",
        "    target_size=(64, 64),\n",
        "    batch_size=32,\n",
        "    class_mode=\"binary\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mrCMmGw9pHys"
      },
      "source": [
        "### Preprocessing the Test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
        "\n",
        "test_set = test_datagen.flow_from_directory(\n",
        "    \"./filez_cats_dogs/test_set/\",\n",
        "    target_size=(64, 64),\n",
        "    batch_size=32,\n",
        "    class_mode=\"binary\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "af8O4l90gk7B"
      },
      "source": [
        "## Part 2 - Building the CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ces1gXY2lmoX"
      },
      "source": [
        "### Initialising the CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn = tf.keras.models.Sequential()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u5YJj_XMl5LF"
      },
      "source": [
        "### Step 1 - Convolution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adding a convolutional layer to a CNN using TensorFlow's Keras API\n",
        "\n",
        "**Params**:\n",
        "\n",
        "- `cnn.add(...)`: This method is called on a Sequential model object, and it adds a layer to the neural network.\n",
        "\n",
        "- `tf.keras.layers.Conv2D(...)`: This specifies that the layer being added is a 2D convolutional layer, which is suitable for processing images that have height and width dimensions.\n",
        "\n",
        "- `filters=32`: The convolutional layer will have 32 filters (or kernels). Each filter is responsible for capturing some specific feature from the image, like edges, textures, or more complex patterns. Having 32 filters means the layer will output 32 different feature maps.\n",
        "\n",
        "- `kernel_size=3`: This defines the size of the filter window that will scan over the image. 3 here means a 3x3 grid. A smaller kernel size can capture finer details, while larger kernels capture more global features.\n",
        "\n",
        "- `activation=\"relu\"`: The activation function used here is the Rectified Linear Unit (ReLU). It introduces non-linearity into the model, allowing it to learn more complex patterns. The ReLU function outputs the input directly if it's positive, otherwise, it will output zero.\n",
        "\n",
        "- `input_shape=[64, 64, 3]`: This defines the shape of the input data that the layer will receive. Since the images are 64x64 in size and color (which implies they have 3 color channels: Red, Green, Blue), the input_shape parameter is set to [64, 64, 3]. The input shape is typically only specified in the first layer of the network so it knows the size of the incoming data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn.add(\n",
        "    tf.keras.layers.Conv2D(\n",
        "        filters=32,\n",
        "        kernel_size=3, # 3x3 dimensions for RGB color\n",
        "        activation=\"relu\",\n",
        "        input_shape=[64, 64, 3],  # image size is 64x64 and is in colors (3 for RGB)\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tf87FpvxmNOJ"
      },
      "source": [
        "### Step 2 - Pooling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adding a Max Pooling layer to the CNN. This layer looks at small portions of your image to spot the biggest features while making the image smaller and more manageable for the computer to process.\n",
        "\n",
        "**Params**:\n",
        "\n",
        "- `cnn.add(...)`: This adds a new layer to the CNN model cnn.\n",
        "\n",
        "- `tf.keras.layers.MaxPool2D(...)`: This specifies that the layer being added is a 2D max pooling layer. Max pooling is a form of down-sampling which reduces the spatial dimensions (height and width) of the input volume for the next convolution layer.\n",
        "\n",
        "- `pool_size=2`: This parameter defines the size of the window over which to take the maximum. 2 means that the max pooling window will have a size of 2x2. This is going to look at each 2x2 square of the image and pick the largest value.\n",
        "\n",
        "- `strides=2`: This parameter specifies the “step” size of the window as it slides over the image. A stride of 2 means that the pooling window will move 2 pixels at a time, reducing the size of the output by a factor of 2. It effectively reduces the height and width of the input by half."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xaTOgD8rm4mU"
      },
      "source": [
        "### Adding a second convolutional layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "# same as before but without param `input_shape` in Conv2D\n",
        "cnn.add(\n",
        "    tf.keras.layers.Conv2D(\n",
        "        filters=32,\n",
        "        kernel_size=3,\n",
        "        activation=\"relu\",\n",
        "    )\n",
        ")\n",
        "\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tmiEuvTunKfk"
      },
      "source": [
        "### Step 3 - Flattening"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Flatten layer is used to convert the multi-dimensional output of the previous layers (like the output from convolutional or pooling layers) into a one-dimensional array. This transformation is necessary because the following layers in a typical CNN, like fully connected (Dense) layers, require their input in a flat, one-dimensional format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Flatten())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dAoSECOm203v"
      },
      "source": [
        "### Step 4 - Full Connection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adding a Dense layer with 128 neurons to the model cnn.\n",
        "This layer will take the output (now flattened into a one-dimensional array by the previous Flatten layer) and perform further calculations.\n",
        "The ReLU activation function allows this layer to capture non-linear relationships in the data.\n",
        "Dense layers like this are crucial in a CNN for performing high-level reasoning and making predictions based on the features extracted by the previous convolutional and pooling layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=128, activation=\"relu\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yTldFvbX28Na"
      },
      "source": [
        "### Step 5 - Output Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1 neuron: cat or dog\n",
        "# sigmoid: binary classification\n",
        "cnn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D6XkI90snSDl"
      },
      "source": [
        "## Part 3 - Training the CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vfrFQACEnc6i"
      },
      "source": [
        "### Compiling the CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Configures your CNN to use the Adam optimizer, calculates the loss using binary cross-entropy (appropriate for binary classification problems), and will track the accuracy of the model during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ehS-v3MIpX2h"
      },
      "source": [
        "### Training the CNN on the Training set and evaluating it on the Test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.6742 - accuracy: 0.5924 - val_loss: 0.6497 - val_accuracy: 0.5985\n",
            "Epoch 2/25\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.6166 - accuracy: 0.6685 - val_loss: 0.6271 - val_accuracy: 0.6595\n",
            "Epoch 3/25\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.5719 - accuracy: 0.7089 - val_loss: 0.5421 - val_accuracy: 0.7310\n",
            "Epoch 4/25\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.5430 - accuracy: 0.7250 - val_loss: 0.5370 - val_accuracy: 0.7425\n",
            "Epoch 5/25\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.5122 - accuracy: 0.7471 - val_loss: 0.5497 - val_accuracy: 0.7275\n",
            "Epoch 6/25\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.4898 - accuracy: 0.7615 - val_loss: 0.5337 - val_accuracy: 0.7370\n",
            "Epoch 7/25\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.4606 - accuracy: 0.7794 - val_loss: 0.4781 - val_accuracy: 0.7825\n",
            "Epoch 8/25\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.4589 - accuracy: 0.7793 - val_loss: 0.4604 - val_accuracy: 0.7895\n",
            "Epoch 9/25\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.4357 - accuracy: 0.7966 - val_loss: 0.4666 - val_accuracy: 0.7875\n",
            "Epoch 10/25\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.4228 - accuracy: 0.8005 - val_loss: 0.5073 - val_accuracy: 0.7725\n",
            "Epoch 11/25\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.4040 - accuracy: 0.8109 - val_loss: 0.4696 - val_accuracy: 0.7830\n",
            "Epoch 12/25\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.3897 - accuracy: 0.8207 - val_loss: 0.4790 - val_accuracy: 0.7775\n",
            "Epoch 13/25\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.3743 - accuracy: 0.8331 - val_loss: 0.4546 - val_accuracy: 0.8035\n",
            "Epoch 14/25\n",
            "250/250 [==============================] - 16s 66ms/step - loss: 0.3651 - accuracy: 0.8350 - val_loss: 0.4559 - val_accuracy: 0.7935\n",
            "Epoch 15/25\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.3444 - accuracy: 0.8479 - val_loss: 0.4791 - val_accuracy: 0.7970\n",
            "Epoch 16/25\n",
            "250/250 [==============================] - 17s 67ms/step - loss: 0.3356 - accuracy: 0.8503 - val_loss: 0.4454 - val_accuracy: 0.8110\n",
            "Epoch 17/25\n",
            "250/250 [==============================] - 16s 66ms/step - loss: 0.3147 - accuracy: 0.8634 - val_loss: 0.5239 - val_accuracy: 0.7895\n",
            "Epoch 18/25\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.3048 - accuracy: 0.8739 - val_loss: 0.4925 - val_accuracy: 0.8005\n",
            "Epoch 19/25\n",
            "250/250 [==============================] - 17s 67ms/step - loss: 0.2843 - accuracy: 0.8788 - val_loss: 0.5038 - val_accuracy: 0.8055\n",
            "Epoch 20/25\n",
            "250/250 [==============================] - 17s 67ms/step - loss: 0.2746 - accuracy: 0.8855 - val_loss: 0.5122 - val_accuracy: 0.8055\n",
            "Epoch 21/25\n",
            "250/250 [==============================] - 17s 70ms/step - loss: 0.2488 - accuracy: 0.8957 - val_loss: 0.5281 - val_accuracy: 0.8000\n",
            "Epoch 22/25\n",
            "250/250 [==============================] - 17s 67ms/step - loss: 0.2449 - accuracy: 0.8988 - val_loss: 0.5327 - val_accuracy: 0.8040\n",
            "Epoch 23/25\n",
            "250/250 [==============================] - 17s 67ms/step - loss: 0.2227 - accuracy: 0.9043 - val_loss: 0.5787 - val_accuracy: 0.7965\n",
            "Epoch 24/25\n",
            "250/250 [==============================] - 16s 65ms/step - loss: 0.2054 - accuracy: 0.9171 - val_loss: 0.5728 - val_accuracy: 0.8010\n",
            "Epoch 25/25\n",
            "250/250 [==============================] - 17s 66ms/step - loss: 0.2001 - accuracy: 0.9212 - val_loss: 0.5345 - val_accuracy: 0.8025\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x16f7c4b10>"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# fine-tune the epochs depending on the results. eg: start with 10 and keep increasing.\n",
        "cnn.fit(x=training_set, validation_data=test_set, epochs=25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "U3PZasO0006Z"
      },
      "source": [
        "## Part 4 - Making a single prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'cats': 0, 'dogs': 1}"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_set.class_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 11ms/step\n",
            "image is a cat\n",
            "1/1 [==============================] - 0s 12ms/step\n",
            "image is a dog\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "\n",
        "\n",
        "def predict(image_num: int):\n",
        "    test_image = image.load_img(\n",
        "        f\"./filez_cats_dogs/single_prediction/cat_or_dog_0{image_num}.jpg\",\n",
        "        target_size=(64, 64),\n",
        "    )\n",
        "\n",
        "    test_image = image.img_to_array(test_image)\n",
        "    test_image = np.expand_dims(test_image, axis=0)\n",
        "    result = cnn.predict(test_image)\n",
        "    if result[0][0] == 1:\n",
        "        prediction = \"dog\"\n",
        "    else:\n",
        "        prediction = \"cat\"\n",
        "\n",
        "    print(f\"image is a {prediction}\")\n",
        "\n",
        "\n",
        "predict(1) # cat\n",
        "predict(2) # dog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "convolutional_neural_network.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
