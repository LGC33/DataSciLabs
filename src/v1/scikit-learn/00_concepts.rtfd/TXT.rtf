{\rtf1\ansi\ansicpg1252\cocoartf2758
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww21060\viewh12940\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 Bias [simplicity]: refers to the error that is introduced by approximating a real-world problem, which\
    may be complex, by a much simpler model. In machine learning, high bias can cause an algorithm to\
    miss the relevant relations between features and target outputs (underfitting).\
\
Variance [complexity]: the error that is due to too much complexity in the learning algorithm, which leads\
    to the model capturing noise in the training data. High variance can cause overfitting, where the model\
    learns the noise in the training data to the extent that it negatively impacts the performance of the\
    model on new data.\
\
Bias-Variance Tradeoff: is the balance between the complexity of the model (variance) and the simplicity of\
    the model (bias). Ideally, one wants to choose a model that appropriately captures the regularities in\
    its training data but also generalizes well to unseen data.\
    - If a model is too simple and has very few parameters, it may have high bias and low variance.\
    - On the other hand, a model that has many parameters may fit the training data very well but also have\
      high variance and low bias.\
    The goal in tuning a model is to find the right balance between these two such that the model performs\
    optimally on new, unseen data.\
\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Pasted Graphic.png \width11980 \height11660 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}}