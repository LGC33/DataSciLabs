{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition\n",
    "\n",
    "**Multiple linear regression** is an extension of simple linear regression, used to model the relationship between two or more independent variables (inputs) and a single dependent variable (output) by fitting a linear equation to the observed data.\n",
    "\n",
    "### Formula:\n",
    "\n",
    "The formula for multiple linear regression expands on the simple linear regression formula to accommodate multiple independent variables:\n",
    "\n",
    "$y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n + \\epsilon$\n",
    "\n",
    "Here's what each element in the formula represents:\n",
    "\n",
    "- `y`: The dependent variable, target variable, or the output that we're trying to predict.\n",
    "- `Œ≤0`: The constant or intercept. It represents the predicted value of \\( y \\) when all the independent variables are zero.\n",
    "- `Œ≤1`, `Œ≤2`, ..., `Œ≤n`: These are the coefficients of the independent variables. Each coefficient represents the change in the predicted \\( y \\) value for a one-unit change in the corresponding independent variable, holding all other variables constant.\n",
    "- `x1`, `x2`, ..., `xn`: The independent variables or predictors. These are the variables we think have an effect on our dependent variable.\n",
    "- `ùüÑ`: The error term or residual. It represents the part of \\( y \\) not explained by the model, capturing all other factors influencing \\( y \\) but not included in the model.\n",
    "\n",
    "### Key Differences from Simple Linear Regression:\n",
    "\n",
    "- **Number of Predictors**: Involves multiple independent variables compared to one in simple linear regression.\n",
    "- **Interpretation of Coefficients**: Each coefficient must be interpreted while holding all other variables constant, which can be more complex than in simple linear regression.\n",
    "- **Assumptions**: Additional assumptions are required, like no perfect multicollinearity among the independent variables.\n",
    "- **Model Complexity**: The model is more complex and requires more data to estimate the coefficients reliably.\n",
    "\n",
    "### Example\n",
    "\n",
    "Imagine we want to predict a house‚Äôs selling price (our dependent variable \\( y \\)) based on its size, age, and location. Here, the independent variables (\\( x_1, x_2, x_3 \\)) would be the size of the house, its age, and a numerical value representing the location.\n",
    "\n",
    "- `Œ≤1`, `Œ≤2`, and `Œ≤3` would tell us how much we expect the selling price to change with a one-unit change in house size, age, and location value, respectively, holding other factors constant.\n",
    "- `ùüÑ` accounts for the variation in selling price not explained by house size, age, and location.\n",
    "\n",
    "### Calculation\n",
    "\n",
    "Just like in simple linear regression, the coefficients in multiple linear regression are typically estimated using Ordinary Least Squares (OLS). However, the calculation is more complex due to the involvement of multiple variables. The goal remains to minimize the sum of the squares of the residuals, but the calculation involves solving a set of linear equations or using matrix algebra.\n",
    "\n",
    "### Multicollinearity Consideration\n",
    "\n",
    "An important consideration in multiple linear regression is multicollinearity, where two or more independent variables are highly correlated with each other. This can make it difficult to determine the individual effect of each independent variable on the dependent variable and can lead to unreliable coefficient estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1b8fd1e7-2a7f-48ea-9bf0-2e4aa8d36cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R&amp;D Spend</th>\n",
       "      <th>Administration</th>\n",
       "      <th>Marketing Spend</th>\n",
       "      <th>State</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>165349.20</td>\n",
       "      <td>136897.80</td>\n",
       "      <td>471784.10</td>\n",
       "      <td>New York</td>\n",
       "      <td>192261.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>162597.70</td>\n",
       "      <td>151377.59</td>\n",
       "      <td>443898.53</td>\n",
       "      <td>California</td>\n",
       "      <td>191792.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>153441.51</td>\n",
       "      <td>101145.55</td>\n",
       "      <td>407934.54</td>\n",
       "      <td>Florida</td>\n",
       "      <td>191050.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>144372.41</td>\n",
       "      <td>118671.85</td>\n",
       "      <td>383199.62</td>\n",
       "      <td>New York</td>\n",
       "      <td>182901.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>142107.34</td>\n",
       "      <td>91391.77</td>\n",
       "      <td>366168.42</td>\n",
       "      <td>Florida</td>\n",
       "      <td>166187.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   R&D Spend  Administration  Marketing Spend       State     Profit\n",
       "0  165349.20       136897.80        471784.10    New York  192261.83\n",
       "1  162597.70       151377.59        443898.53  California  191792.06\n",
       "2  153441.51       101145.55        407934.54     Florida  191050.39\n",
       "3  144372.41       118671.85        383199.62    New York  182901.99\n",
       "4  142107.34        91391.77        366168.42     Florida  166187.94"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv(\"./filez/50_Startups.csv\")\n",
    "\n",
    "X = dataset.iloc[:, :-1].values  # features\n",
    "y = dataset.iloc[:, -1].values  # target\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0, 1.0, 165349.2, 136897.8, 471784.1],\n",
       "       [0.0, 0.0, 162597.7, 151377.59, 443898.53],\n",
       "       [1.0, 0.0, 153441.51, 101145.55, 407934.54],\n",
       "       [0.0, 1.0, 144372.41, 118671.85, 383199.62],\n",
       "       [1.0, 0.0, 142107.34, 91391.77, 366168.42]], dtype=object)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ct = ColumnTransformer(\n",
    "    transformers=[(\"encoder\", OneHotEncoder(drop='first'), [3])], remainder=\"passthrough\"\n",
    ")\n",
    "X = np.array(ct.fit_transform(X))\n",
    "\n",
    "X[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ‚òùüèª No need to apply scaling in Multiple Linear Regression with Ordinary Least Squares (OLS):\n",
    "- **Effect on Coefficients**: OLS estimates are scale-invariant. This means that whether the features are scaled or not, the model will produce the same predictions. The coefficients will adjust their scale automatically to accommodate the scale of the features.\n",
    "\n",
    "- **Intercept Term**: The OLS implementation in sklearn includes an intercept term that compensates for differences in scale among the features, ensuring that the model provides an accurate fit to the data.\n",
    "\n",
    "- **Optimization Algorithm**: OLS does not use gradient descent or any other iterative optimization algorithm that would benefit from feature scaling. It solves the regression equation directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ‚òùüèª Do we need to check the linear regression assumptions beforehand? Nope!\n",
    "- It's preferrable to apply all models and evaluate them. If our dataset is not linear, this will be shown with poor performance when using regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting dataset into Train/Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ‚òùüèª Do we need to select the backwards elimination to select features with highest p.values? Nope!\n",
    "- The model will automatically identify the best features (more significant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Multiple Linear Regression model with Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the Test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[103015.2  103282.38]\n",
      " [132582.28 144259.4 ]\n",
      " [132447.74 146121.95]\n",
      " [ 71976.1   77798.83]\n",
      " [178537.48 191050.39]\n",
      " [116161.24 105008.31]\n",
      " [ 67851.69  81229.06]\n",
      " [ 98791.73  97483.56]\n",
      " [113969.44 110352.25]\n",
      " [167921.07 166187.94]]\n"
     ]
    }
   ],
   "source": [
    "# vector of the predicted profits\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# show only 2 decimals\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# show predicted profits (left) and actual profits (right)\n",
    "# display vertically -> y_pred.reshape(rows, cols)\n",
    "print(\n",
    "    np.concatenate(\n",
    "        (y_pred.reshape(len(y_pred), 1), y_test.reshape(len(y_test), 1)), axis=1\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a new prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New prediction: 189,416.58\n"
     ]
    }
   ],
   "source": [
    "input_features = [\n",
    "    0,  # Dummy 1\n",
    "    0,  # Dummy 2\n",
    "    170000,  # R&D Spend\n",
    "    50000,  # Administration\n",
    "    375000,  # Marketing Spend\n",
    "]\n",
    "\n",
    "print(f\"New prediction: {regressor.predict([input_features])[0]:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 7,514.29\n",
      "MSE: 83,502,864.03\n",
      "R^2: 0.93\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# MAE: Mean Absolute Error\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f'MAE: {mae:,.2f}')\n",
    "\n",
    "# MSE: Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'MSE: {mse:,.2f}')\n",
    "\n",
    "# R-squared Score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'R^2: {r2:,.2f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An MAE of 7,514.29 means that, on average, the predictions of our model are about $7,514.29 away from the actual values.\n",
    "- An R^2 of 0.93 is quite high, indicating that 93% of the variance in your dependent variable is predictable from the independent variables (it can sometimes indicate overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the linear regression equation with coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept (B0):        42,554.17\n",
      "coefficients (B1..BN): -959.28, 699.37, 0.77, 0.03, 0.04\n"
     ]
    }
   ],
   "source": [
    "print(f'intercept (B0):        {regressor.intercept_:,.2f}')\n",
    "print(f\"coefficients (B1..BN): {', '.join(f'{coef:,.2f}' for coef in regressor.coef_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression formula:\n",
    "\n",
    "$y = \\beta_0 + \\beta_1x_1 ... + \\beta_Nx_N + \\epsilon$\n",
    "\n",
    "In our context:\n",
    "\n",
    "$Profit = 42,554.17 - 959.28√ódummy_state_1 + 699.37√ódumm_state_2 + 0.77√órd_spend + 0.03√óadministration + 0.04√ómarketing_spend$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO : BACKWARD ELIMINATION"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
