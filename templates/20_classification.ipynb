{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b15b9bae",
   "metadata": {},
   "source": [
    "# Classification Template\n",
    "v1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cf900c",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1824,
   "id": "75a53813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "RANDOM_STATE = 101"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26b7192",
   "metadata": {},
   "source": [
    "### Read Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1825,
   "id": "7aa0892d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_data(df):\n",
    "    \"\"\"\n",
    "    Display the first two and the last two records of a DataFrame\n",
    "    \"\"\"\n",
    "    print(pd.concat([df.head(2), df.tail(2)]))\n",
    "\n",
    "\n",
    "def show_missing_data(df):\n",
    "    \"\"\"\n",
    "    Display number and percentage of missing values in all columns\n",
    "    \"\"\"\n",
    "    total = df.isnull().sum().sort_values(ascending=False)\n",
    "    percent = (\n",
    "        ((df.isnull().sum() / df.isnull().count()) * 100)\n",
    "        .sort_values(ascending=False)\n",
    "        .round(2)\n",
    "    )\n",
    "    missing_data = pd.concat([total, percent], axis=1, keys=[\"# missing\", \"% missing\"])\n",
    "    print(missing_data)\n",
    "\n",
    "\n",
    "def show_unique_values(df, fields):\n",
    "    \"\"\"\n",
    "    Show unique values in DataFrame given a list of fields\n",
    "    \"\"\"\n",
    "    for field in fields:\n",
    "        try:\n",
    "            print(f\"{field}: {df[field].unique()}\")\n",
    "        except KeyError:\n",
    "            print(f\"`{field}` not found in DataFrame\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372aceaf",
   "metadata": {},
   "source": [
    "### Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1826,
   "id": "a09e1be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "def evaluate_model(classifier, X_test, y_test):\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    print(f\"1) classification_report:\\n\\n\", classification_report(y_test, y_pred))\n",
    "    print(f\"2) confusion_matrix:\\n\\n\", confusion_matrix(y_test, y_pred), \"\\n\")\n",
    "    print(f\"3) accuracy_score:\\n\\n\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1827,
   "id": "6abbeba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_model_kfold(classifiers, X_train, y_train, cv=10):\n",
    "def evaluate_model_kfold(classifiers, _X_train, _X_train_scaled, y_train, models_to_scale, cv=10):\n",
    "    \"\"\"\n",
    "    TBD\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for classifier in classifiers:\n",
    "\n",
    "        if type(classifier).__name__ in models_to_scale:\n",
    "            X_train = _X_train_scaled\n",
    "        else:\n",
    "            X_train = _X_train\n",
    "\n",
    "        accuracies = cross_val_score(estimator=classifier, X=X_train, y=y_train, cv=cv)\n",
    "        results.append({\n",
    "            \"Model\": type(classifier).__name__,\n",
    "            \"Accuracy\": accuracies.mean() * 100,\n",
    "            \"SD\": accuracies.std() * 100\n",
    "        })\n",
    "\n",
    "    # Sort the results by accuracy in descending order\n",
    "    sorted_results = sorted(results, key=lambda x: x[\"Accuracy\"], reverse=True)\n",
    "\n",
    "    # Print the results in a table format\n",
    "    print(f\"{'Model':<25} {'Accuracy %':<12} {'SD %':<10}\")\n",
    "    print('-' * 43)\n",
    "    for result in sorted_results:\n",
    "        print(f\"{result['Model']:<25} {result['Accuracy']:<12.2f} {result['SD']:<10.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1828,
   "id": "67faed75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "def evaluate_model_grid_search_cv(\n",
    "    classifier, X_train, y_train, params, scoring=\"accuracy\", cv=10, n_jobs=-1\n",
    "):\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=classifier, param_grid=params, scoring=scoring, cv=cv, n_jobs=n_jobs\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X=X_train, y=y_train)\n",
    "\n",
    "    best_accuracy = grid_search.best_score_\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    print(f\"Model: {type(classifier).__name__}\")\n",
    "    print(f\" - Best accuracy: {best_accuracy * 100:,.2f}%\")\n",
    "    print(f\" - Best params: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1be081",
   "metadata": {},
   "source": [
    "### Write Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1829,
   "id": "3b90320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "def update_null_values(df, fields, strategy, fill_value=np.nan):\n",
    "    \"\"\"\n",
    "    Update values with a given strategy.\n",
    "    `fill_value` is only used when `strategy` = \"constant\"\n",
    "    `strategy`:  {'constant', 'most_frequent', 'mean', 'median'}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        imputer = SimpleImputer(\n",
    "            missing_values=np.nan, strategy=strategy, fill_value=fill_value\n",
    "        )\n",
    "        imputer.fit(df[fields])\n",
    "        df_transformed = df.copy()\n",
    "        df_transformed[fields] = imputer.transform(df[fields])\n",
    "        return df_transformed\n",
    "    except ValueError as e:\n",
    "        print(f\"❌ Error: {e}\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1830,
   "id": "5ed6387e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "def encode_categorical_data(df, fields, encoder):\n",
    "    \"\"\"\n",
    "    Function to encode categorical data in a DataFrame:\n",
    "    - OneHot: tbd\n",
    "    - Dummy: tbd\n",
    "    - Label: tbd\n",
    "    @TODO: explain when applying each one\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if encoder == \"OneHot\":\n",
    "            # Create a ColumnTransformer, applying OneHotEncoder to specified fields\n",
    "            ct = ColumnTransformer(\n",
    "                transformers=[(\"encoder\", OneHotEncoder(), fields)],\n",
    "                remainder=\"passthrough\",\n",
    "            )\n",
    "            # Apply ColumnTransformer, resulting in an array\n",
    "            transformed_data = ct.fit_transform(df)\n",
    "            # Create new column names for the one-hot encoded columns\n",
    "            encoded_columns = ct.named_transformers_[\"encoder\"].get_feature_names_out(\n",
    "                fields\n",
    "            )\n",
    "            # Combine the new column names with the non-transformed columns\n",
    "            non_transformed_columns = [col for col in df.columns if col not in fields]\n",
    "            new_column_names = list(encoded_columns) + non_transformed_columns\n",
    "            # Create a DataFrame from the transformed data\n",
    "            df_transformed = pd.DataFrame(\n",
    "                transformed_data, columns=new_column_names, index=df.index\n",
    "            )\n",
    "\n",
    "        elif encoder == \"Dummy\":\n",
    "            # Create dummy variables\n",
    "            dummies = pd.get_dummies(df[fields], drop_first=True)\n",
    "            # Drop the original fields and concatenate the dummy variables\n",
    "            df_transformed = pd.concat([df.drop(fields, axis=1), dummies], axis=1)\n",
    "\n",
    "        elif encoder == \"Label\":\n",
    "            df_transformed = df.copy()\n",
    "            # update original target fields with 0-N categorical values\n",
    "            for field in fields:\n",
    "                le = LabelEncoder()\n",
    "                df_transformed[field] = le.fit_transform(df_transformed[field])\n",
    "        else:\n",
    "            print(f\"encoder `{encoder}` not found\")\n",
    "            return df\n",
    "\n",
    "        return df_transformed\n",
    "    except KeyError as e:\n",
    "        print(f\"❌ Error: {e}\")\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1831,
   "id": "250d0ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "\n",
    "\n",
    "def scale_features(X_train, X_test, fields):\n",
    "    \"\"\"\n",
    "    - Only for non-dummy numerical features\n",
    "    - For KNN, SVM or Logistic Reg/Linear Reg/NN with Gradient descent optimisation\n",
    "    - For classification, no need to scale dependent variable\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create copies of the original DataFrames\n",
    "        X_train_scaled, X_test_scaled = X_train.copy(), X_test.copy()\n",
    "\n",
    "        # Scale only the specified fields\n",
    "        X_train_scaled[fields] = sc.fit_transform(X_train[fields])\n",
    "        X_test_scaled[fields] = sc.transform(X_test[fields])\n",
    "\n",
    "        return X_train_scaled, X_test_scaled\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")\n",
    "        return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ef55dc",
   "metadata": {},
   "source": [
    "### Regression Model Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aca1040",
   "metadata": {},
   "source": [
    "@TODO: include random_state in all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1832,
   "id": "543ca9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def fit_logistic_regression(\n",
    "    X_train, y_train, c=1.0, solver=\"lbfgs\", penalty=\"l2\", max_iter=1000\n",
    "):\n",
    "    lr = LogisticRegression(\n",
    "        random_state=RANDOM_STATE,\n",
    "        max_iter=max_iter,\n",
    "        C=c,\n",
    "        solver=solver,\n",
    "        penalty=penalty,\n",
    "    )\n",
    "    lr.fit(X_train, y_train)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1833,
   "id": "eb1dfb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "def fit_knn(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    n_neighbors=5,\n",
    "    weights=\"uniform\",\n",
    "    algorithm=\"auto\",\n",
    "    p=2,\n",
    "    leaf_size=30,\n",
    "):\n",
    "    knn = KNeighborsClassifier(\n",
    "        n_neighbors=n_neighbors,\n",
    "        weights=weights,\n",
    "        algorithm=algorithm,\n",
    "        p=p,\n",
    "        leaf_size=leaf_size,\n",
    "    )\n",
    "    knn.fit(X_train, y_train)\n",
    "    return knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1834,
   "id": "9e3f9591",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "def fit_svc(X_train, y_train, kernel=\"rbf\", c=1.0, gamma=\"scale\", degree=3):\n",
    "    svc = SVC(kernel=kernel, random_state=RANDOM_STATE, C=c, gamma=gamma, degree=degree)\n",
    "    svc.fit(X_train, y_train)\n",
    "    return svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1835,
   "id": "ffae0217",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "def fit_nb(X_train, y_train, var_smoothing=1e-9):\n",
    "    nb = GaussianNB(var_smoothing=var_smoothing)\n",
    "    nb.fit(X_train, y_train)\n",
    "    return nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1836,
   "id": "2432b0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "# TODO: default criterion: gini vs. entropy\n",
    "def fit_decision_tree(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    criterion=\"entropy\",\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    max_features=None,\n",
    "    max_leaf_nodes=None,\n",
    "    splitter=\"best\",\n",
    "):\n",
    "    dt = DecisionTreeClassifier(\n",
    "        random_state=RANDOM_STATE,\n",
    "        criterion=criterion,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_features=max_features,\n",
    "        max_leaf_nodes=max_leaf_nodes,\n",
    "        splitter=splitter,\n",
    "    )\n",
    "    dt.fit(X_train, y_train)\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1837,
   "id": "0e96b5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# TODO: default criterion: gini vs. entropy\n",
    "def fit_random_forest(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    n_estimators=100,\n",
    "    criterion=\"entropy\",\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    max_features=\"sqrt\",\n",
    "    bootstrap=True,\n",
    "    class_weight=None,\n",
    "):\n",
    "    rf = RandomForestClassifier(\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_estimators=n_estimators,\n",
    "        criterion=criterion,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_features=max_features,\n",
    "        bootstrap=bootstrap,\n",
    "        class_weight=class_weight,\n",
    "    )\n",
    "    rf.fit(X_train, y_train)\n",
    "    return rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1838,
   "id": "a2ce24ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "def fit_xgboost(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    learning_rate=0.3,\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    min_child_weight=1,\n",
    "    gamma=0,\n",
    "    subsample=1,\n",
    "    colsample_bytree=1,\n",
    "):\n",
    "    xgb = XGBClassifier(\n",
    "        random_state=RANDOM_STATE,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric=\"logloss\",\n",
    "        learning_rate=learning_rate,\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_child_weight=min_child_weight,\n",
    "        gamma=gamma,\n",
    "        subsample=subsample,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "    )\n",
    "    xgb.fit(X_train, y_train)\n",
    "    return xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1839,
   "id": "f25dc385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "\n",
    "def fit_catboost(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_metric=\"Logloss\",\n",
    "    learning_rate=0.009,\n",
    "    depth=6,\n",
    "    l2_leaf_reg=3,\n",
    "    iterations=1000,\n",
    "    border_count=254,\n",
    "    bootstrap_type=\"MVS\",\n",
    "    subsample=0.8,\n",
    "):\n",
    "    cat = CatBoostClassifier(\n",
    "        random_state=RANDOM_STATE,\n",
    "        verbose=0,\n",
    "        eval_metric=eval_metric,\n",
    "        learning_rate=learning_rate,\n",
    "        depth=depth,\n",
    "        l2_leaf_reg=l2_leaf_reg,\n",
    "        iterations=iterations,\n",
    "        border_count=border_count,\n",
    "        bootstrap_type=bootstrap_type,\n",
    "        subsample=subsample,\n",
    "    )\n",
    "    cat.fit(X_train, y_train)\n",
    "    return cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb160f5",
   "metadata": {},
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1840,
   "id": "96f16ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PassengerId  Survived  Pclass  \\\n",
      "0              1         0       3   \n",
      "1              2         1       1   \n",
      "889          890         1       1   \n",
      "890          891         0       3   \n",
      "\n",
      "                                                  Name     Sex   Age  SibSp  \\\n",
      "0                              Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "889                              Behr, Mr. Karl Howell    male  26.0      0   \n",
      "890                                Dooley, Mr. Patrick    male  32.0      0   \n",
      "\n",
      "     Parch     Ticket     Fare Cabin Embarked  \n",
      "0        0  A/5 21171   7.2500   NaN        S  \n",
      "1        0   PC 17599  71.2833   C85        C  \n",
      "889      0     111369  30.0000  C148        C  \n",
      "890      0     370376   7.7500   NaN        Q  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../src/v1/07_scikit-learn/filez/titanic_train.csv')\n",
    "show_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9480465d",
   "metadata": {},
   "source": [
    "- field_1: description_1. Explanation.\n",
    "- field_2: description_2. Explanation.\n",
    "- field_3: description_3. Explanation.\n",
    "- field_4: description_4 (0 = No, 1 = Yes). Explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e37dc16",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1841,
   "id": "ee3b3f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# Display DataFrame info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1842,
   "id": "64500f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             # missing  % missing\n",
      "Cabin              687      77.10\n",
      "Age                177      19.87\n",
      "Embarked             2       0.22\n",
      "PassengerId          0       0.00\n",
      "Survived             0       0.00\n",
      "Pclass               0       0.00\n",
      "Name                 0       0.00\n",
      "Sex                  0       0.00\n",
      "SibSp                0       0.00\n",
      "Parch                0       0.00\n",
      "Ticket               0       0.00\n",
      "Fare                 0       0.00\n"
     ]
    }
   ],
   "source": [
    "# Display number and percentage of missing values\n",
    "show_missing_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1843,
   "id": "37bd9c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
       "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
       "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
       "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
       "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
       "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
       "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
       "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
       "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
       "\n",
       "            Parch        Fare  \n",
       "count  891.000000  891.000000  \n",
       "mean     0.381594   32.204208  \n",
       "std      0.806057   49.693429  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    7.910400  \n",
       "50%      0.000000   14.454200  \n",
       "75%      0.000000   31.000000  \n",
       "max      6.000000  512.329200  "
      ]
     },
     "execution_count": 1843,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Describe statistics on numerical fields\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1844,
   "id": "ccd14e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex: ['male' 'female']\n",
      "Embarked: ['S' 'C' 'Q' nan]\n"
     ]
    }
   ],
   "source": [
    "# show unique values given a list of df fields\n",
    "show_unique_values(df, ['Sex', 'Embarked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1845,
   "id": "75a00c13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embarked\n",
       "S    644\n",
       "C    168\n",
       "Q     77\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 1845,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show count for each value of field 'Embarked'\n",
    "df['Embarked'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1846,
   "id": "9340d0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pclass\n",
       "1    38.233441\n",
       "2    29.877630\n",
       "3    25.140620\n",
       "Name: Age, dtype: float64"
      ]
     },
     "execution_count": 1846,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('Pclass')['Age'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1847,
   "id": "8ce26a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@TODO: seaborn charts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da2900e",
   "metadata": {},
   "source": [
    "### Data cleaning & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27631d2c",
   "metadata": {},
   "source": [
    "    TO-BE-REMOVED\n",
    "- remove or update null values\n",
    "- manage outliers\n",
    "- drop irrelevant fields (i.e.: ids, names, ..)\n",
    "- correct data entry errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1848,
   "id": "01bd099d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update null values\n",
    "# 1) Age\n",
    "mean_age_per_class = df.groupby('Pclass')['Age'].transform('mean')\n",
    "df['Age'] = df['Age'].fillna(mean_age_per_class)\n",
    "# 2) Embarked\n",
    "df = update_null_values(df=df, strategy=\"constant\", fields=[\"Embarked\"], fill_value=\"S\")\n",
    "\n",
    "# Remove unnecessary fields\n",
    "df = df.drop([\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6c0e8f",
   "metadata": {},
   "source": [
    "### Encoding categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1849,
   "id": "644e123d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Sex_male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass   Age  SibSp  Parch     Fare  Embarked_Q  Embarked_S  \\\n",
       "0         0       3  22.0      1      0   7.2500       False        True   \n",
       "1         1       1  38.0      1      0  71.2833       False       False   \n",
       "\n",
       "   Sex_male  \n",
       "0      True  \n",
       "1     False  "
      ]
     },
     "execution_count": 1849,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OneHot encoder - binary values / keep all values\n",
    "# df = encode_categorical_data(df=df, fields=['Embarked', 'Sex'], encoder='OneHot')\n",
    "# Dummy encoder - binary values / remove first value\n",
    "df = encode_categorical_data(df=df, fields=['Embarked', 'Sex'], encoder='Dummy')\n",
    "# Label encoder - integer values 0-N / keep all values\n",
    "# df = encode_categorical_data(df=df, fields=['Embarked', 'Sex'], encoder='Label')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4759ec0a",
   "metadata": {},
   "source": [
    "### Splitting dataset into Train/Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1850,
   "id": "1ffab653",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(\"Survived\", axis=1)\n",
    "y = df[\"Survived\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f21f0dd",
   "metadata": {},
   "source": [
    "### Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1851,
   "id": "3baca652",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_to_scale = [\"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Fare\"]\n",
    "\n",
    "X_train_scaled, X_test_scaled = scale_features(\n",
    "    X_train=X_train, X_test=X_test, fields=fields_to_scale\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea988e0",
   "metadata": {},
   "source": [
    "### Fit Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1852,
   "id": "a4f80ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogReg requires scaling to avoid ConvergenceWarning issue with kfold\n",
    "# KNN & SVC require scaling\n",
    "lr = fit_logistic_regression(X_train_scaled, y_train)\n",
    "knn = fit_knn(X_train_scaled, y_train)\n",
    "svc = fit_svc(X_train_scaled, y_train)\n",
    "nb = fit_nb(X_train, y_train)\n",
    "dt = fit_decision_tree(X_train, y_train)\n",
    "rf = fit_random_forest(X_train, y_train)\n",
    "xgb = fit_xgboost(X_train, y_train)\n",
    "cat = fit_catboost(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ab8eb6",
   "metadata": {},
   "source": [
    "### Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1853,
   "id": "acf9e3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model                     Accuracy %   SD %      \n",
      "-------------------------------------------\n",
      "CatBoostClassifier        81.75        5.41      \n",
      "SVC                       81.60        4.82      \n",
      "RandomForestClassifier    80.33        4.79      \n",
      "LogisticRegression        80.21        4.86      \n",
      "XGBClassifier             80.20        3.80      \n",
      "KNeighborsClassifier      78.37        3.76      \n",
      "GaussianNB                78.10        5.46      \n",
      "DecisionTreeClassifier    76.41        4.84      \n"
     ]
    }
   ],
   "source": [
    "models = [lr, knn, svc, nb, dt, rf, xgb, cat]\n",
    "models_to_scale = [\"KNeighborsClassifier\", \"SVC\", \"LogisticRegression\"]\n",
    "evaluate_model_kfold(models, X_train, X_train_scaled, y_train, models_to_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c726ef9d",
   "metadata": {},
   "source": [
    "### Tune Models\n",
    "*hyperparameters*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1854,
   "id": "947f66c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression\n",
      " - Best accuracy: 80.35%\n",
      " - Best params: {'C': 0.1, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'lbfgs'}\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "params = {\n",
    "    \"C\": [0.01, 0.1, 1, 10, 100],\n",
    "    \"penalty\": [\"l2\"],\n",
    "    \"solver\": [\"lbfgs\", \"newton-cg\", \"sag\"],\n",
    "    \"max_iter\": [1000],\n",
    "}\n",
    "\n",
    "# Feature scaling to avoid ConvergenceWarning issue\n",
    "evaluate_model_grid_search_cv(lr, X_train_scaled, y_train, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1855,
   "id": "a352ffc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: KNeighborsClassifier\n",
      " - Best accuracy: 80.90%\n",
      " - Best params: {'algorithm': 'auto', 'leaf_size': 20, 'n_neighbors': 10, 'p': 2, 'weights': 'uniform'}\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "\n",
    "params = {\n",
    "    \"n_neighbors\": [3, 5, 7, 10],\n",
    "    \"weights\": [\"uniform\", \"distance\"],\n",
    "    \"algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"],\n",
    "    \"p\": [1, 2],\n",
    "    \"leaf_size\": [20, 30, 35, 40, 45],\n",
    "}\n",
    "\n",
    "evaluate_model_grid_search_cv(knn, X_train_scaled, y_train, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1856,
   "id": "43558b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: SVC\n",
      " - Best accuracy: 81.88%\n",
      " - Best params: {'C': 10, 'degree': 2, 'gamma': 'scale', 'kernel': 'poly'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nFurther options:\\n- with 'poly' and 'sigmoid' kernels, try 'coef0' ranges -> [0, 1] or [0, 10])\\n- with 'poly' kernel, try more granular 'degree' ranges\\n- when dataset is imbalanced (i.e., unequal number of instances in each class),\\n  try 'class_weight' param\\n\""
      ]
     },
     "execution_count": 1856,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVC\n",
    "\n",
    "params = {\n",
    "    \"C\": [0.1, 1, 10, 100],\n",
    "    \"kernel\": [\"rbf\", \"poly\"], # linear\n",
    "    \"gamma\": [\"scale\", \"auto\", 0.1, 1],\n",
    "    \"degree\": [2, 3, 4],\n",
    "}\n",
    "\n",
    "evaluate_model_grid_search_cv(svc, X_train_scaled, y_train, params)\n",
    "\n",
    "\"\"\"\n",
    "Further options:\n",
    "- with 'poly' and 'sigmoid' kernels, try 'coef0' ranges -> [0, 1] or [0, 10])\n",
    "- with 'poly' kernel, try more granular 'degree' ranges\n",
    "- when dataset is imbalanced (i.e., unequal number of instances in each class),\n",
    "  try 'class_weight' param\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1857,
   "id": "5452cf6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: GaussianNB\n",
      " - Best accuracy: 78.24%\n",
      " - Best params: {'var_smoothing': 1e-05}\n"
     ]
    }
   ],
   "source": [
    "# Gaussian Naive Bayes\n",
    "\n",
    "params = {\"var_smoothing\": [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4]}\n",
    "\n",
    "evaluate_model_grid_search_cv(nb, X_train, y_train, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1858,
   "id": "89816140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: DecisionTreeClassifier\n",
      " - Best accuracy: 82.73%\n",
      " - Best params: {'criterion': 'entropy', 'max_depth': 10, 'max_features': 'log2', 'max_leaf_nodes': 30, 'min_samples_leaf': 1, 'min_samples_split': 2, 'splitter': 'best'}\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "\n",
    "params = {\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"max_depth\": [10, 20, 30, None],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"max_features\": [\"sqrt\", \"log2\", None],\n",
    "    \"max_leaf_nodes\": [10, 20, 30, None],\n",
    "    \"splitter\": [\"best\", \"random\"],\n",
    "}\n",
    "\n",
    "evaluate_model_grid_search_cv(dt, X_train, y_train, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1859,
   "id": "2fcaf80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: RandomForestClassifier\n",
      " - Best accuracy: 83.01%\n",
      " - Best params: {'bootstrap': True, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "\n",
    "params = {\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"max_depth\": [10, 20], # 30, None\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 4], # 2\n",
    "    \"max_features\": [\"sqrt\", \"log2\", None],\n",
    "    \"bootstrap\": [True, False],\n",
    "    \"class_weight\": [None], # \"balanced\"\n",
    "}\n",
    "\n",
    "evaluate_model_grid_search_cv(rf, X_train, y_train, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1860,
   "id": "003fb0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: XGBClassifier\n",
      " - Best accuracy: 84.41%\n",
      " - Best params: {'colsample_bytree': 0.8, 'gamma': 0.1, 'learning_rate': 0.1, 'max_depth': 5, 'min_child_weight': 5, 'n_estimators': 100, 'subsample': 0.8}\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "\n",
    "params = {\n",
    "    \"learning_rate\": [0.01, 0.1, 0.2, 0.3],\n",
    "    \"n_estimators\": [100, 150, 200],\n",
    "    \"max_depth\": [3, 5], #default is 6\n",
    "    \"min_child_weight\": [1, 3, 5],\n",
    "    \"gamma\": [0, 0.1, 0.2],\n",
    "    \"subsample\": [0.7, 0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.8, 0.9, 1.0],\n",
    "}\n",
    "\n",
    "evaluate_model_grid_search_cv(xgb, X_train, y_train, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1861,
   "id": "531f82f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: CatBoostClassifier\n",
      " - Best accuracy: 82.87%\n",
      " - Best params: {'bootstrap_type': 'Bernoulli', 'border_count': 32, 'depth': 6, 'eval_metric': 'Logloss', 'iterations': 200, 'l2_leaf_reg': 5, 'learning_rate': 0.1, 'subsample': 0.7}\n"
     ]
    }
   ],
   "source": [
    "# CatBoost\n",
    "\n",
    "params = {\n",
    "    \"eval_metric\": [\"Logloss\"],\n",
    "    \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "    \"depth\": [6, 8, 10],\n",
    "    \"l2_leaf_reg\": [1, 3, 5],\n",
    "    \"iterations\": [100, 200],\n",
    "    \"border_count\": [32, 64, 128],\n",
    "    \"bootstrap_type\": [\"Bernoulli\"],\n",
    "    \"subsample\": [0.7, 0.9],\n",
    "}\n",
    "\n",
    "evaluate_model_grid_search_cv(cat, X_train, y_train, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3f9d55",
   "metadata": {},
   "source": [
    "### Re-evaluate Models with tunned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1869,
   "id": "d4f2ce89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model                     Accuracy %   SD %      \n",
      "-------------------------------------------\n",
      "XGBClassifier             84.41        4.94      \n",
      "RandomForestClassifier    83.01        4.72      \n",
      "CatBoostClassifier        82.87        4.92      \n",
      "DecisionTreeClassifier    82.73        3.10      \n",
      "SVC                       81.88        4.65      \n",
      "KNeighborsClassifier      80.90        4.56      \n",
      "LogisticRegression        80.35        4.07      \n",
      "GaussianNB                78.24        4.78      \n"
     ]
    }
   ],
   "source": [
    "lr = fit_logistic_regression(X_train_scaled, y_train, 0.1, \"lbfgs\", \"l2\", 1000)\n",
    "knn = fit_knn(X_train_scaled, y_train, 10, \"uniform\", \"auto\", 2, 20)\n",
    "svc = fit_svc(X_train_scaled, y_train, \"poly\", 10, \"scale\", 2)\n",
    "nb = fit_nb(X_train, y_train, 1e-05)\n",
    "dt = fit_decision_tree(X_train, y_train, \"entropy\", 10, 2, 1, \"log2\", 30, \"best\")\n",
    "rf = fit_random_forest(X_train, y_train, 50, \"entropy\", 10, 5, 1, \"sqrt\", True, None)\n",
    "xgb = fit_xgboost(X_train, y_train, 0.1, 100, 5, 5, 0.1, 0.8, 0.8)\n",
    "cat = fit_catboost(X_train, y_train, \"Logloss\", 0.1, 6, 5, 200, 32, \"Bernoulli\", 0.7)\n",
    "\n",
    "models = [lr, knn, svc, nb, dt, rf, xgb, cat]\n",
    "models_to_scale = [\"KNeighborsClassifier\", \"SVC\", \"LogisticRegression\"]\n",
    "evaluate_model_kfold(models, X_train, X_train_scaled, y_train, models_to_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a2e920",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1863,
   "id": "911de7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values to predict\n",
    "new_data = pd.DataFrame(\n",
    "    [\n",
    "        [2, 23, 0, 0, 13, False, True, True],\n",
    "        [1, 51, 0, 0, 26.5, False, True, True],\n",
    "        [3, 29, 0, 0, 9.5, False, True, True],\n",
    "        [1, 40, 1, 1, 134.5, False, False, True],\n",
    "        [2, 6, 0, 1, 33, False, True, False],\n",
    "        [3, 19, 0, 0, 14.5, False, True, True],\n",
    "        [3, 32, 0, 0, 56.4958, False, True, True],\n",
    "        [1, 41, 0, 0, 134.5, False, False, False],\n",
    "        [1, 44, 0, 1, 57.9792, False, False, False],\n",
    "        [3, 29.699118, 8, 2, 69.5500, False, True, False],\n",
    "    ],\n",
    "    columns=[\n",
    "        \"Pclass\",\n",
    "        \"Age\",\n",
    "        \"SibSp\",\n",
    "        \"Parch\",\n",
    "        \"Fare\",\n",
    "        \"Embarked_Q\",\n",
    "        \"Embarked_S\",\n",
    "        \"Sex_male\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1864,
   "id": "d88377c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 1 1 0 1 1 1 0] y_train\n",
      "[0 0 1 1 1 0 1 1 1 0] XGBClassifier\n",
      "[0 1 0 0 1 0 0 1 1 0] RandomForestClassifier\n",
      "[0 0 0 0 1 0 1 1 1 0] CatBoostClassifier\n",
      "[0 0 0 0 1 0 0 1 1 0] SVC\n"
     ]
    }
   ],
   "source": [
    "def make_prediction(\n",
    "    new_data: pd.DataFrame, model: any, scaled: bool, fields_to_scale: list\n",
    "):\n",
    "    \"\"\"\n",
    "    TBD\n",
    "    \"\"\"\n",
    "    if scaled and fields_to_scale:\n",
    "        # Separate the data into features that need scaling and those that don't\n",
    "        features_to_scale = new_data[fields_to_scale]\n",
    "        features_not_to_scale = new_data.drop(columns=fields_to_scale)\n",
    "\n",
    "        # Apply scaling only to the required features\n",
    "        scaled_features = sc.transform(features_to_scale)\n",
    "\n",
    "        # Recombine the scaled and unscaled features\n",
    "        new_data_prepared = pd.concat(\n",
    "            [\n",
    "                pd.DataFrame(\n",
    "                    scaled_features, columns=fields_to_scale, index=new_data.index\n",
    "                ),\n",
    "                features_not_to_scale,\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "        new_prediction = model.predict(new_data_prepared)\n",
    "    else:\n",
    "        new_prediction = model.predict(new_data)\n",
    "\n",
    "    print(f\"{new_prediction} {type(model).__name__}\")\n",
    "\n",
    "print(f\"[{' '.join(map(str, y_train.head(5).tolist() + y_train.tail(5).tolist()))}] y_train\")\n",
    "make_prediction(new_data, xgb, False, fields_to_scale)\n",
    "make_prediction(new_data, rf, False, fields_to_scale)\n",
    "make_prediction(new_data, cat, False, fields_to_scale)\n",
    "make_prediction(new_data, svc, True, fields_to_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7b9292",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa7ecb58",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "149f8ed0",
   "metadata": {},
   "source": [
    "TODO: use new df's after every change, but beware of the mem space required.\n",
    "\n",
    "1. **Initial Stages**:\n",
    "   - `df_raw`: The original, unmodified dataset.\n",
    "   - `df_loaded`: Data after initial loading, possibly from multiple sources.\n",
    "\n",
    "2. **Cleaning and Preprocessing**:\n",
    "   - `df_cleaned`: After basic cleaning (removing duplicates, handling missing values).\n",
    "   - `df_filtered`: Data after filtering based on certain criteria.\n",
    "   - `df_imputed`: Where missing values have been imputed.\n",
    "   - `df_deduped`: After removing duplicates.\n",
    "\n",
    "3. **Feature Engineering**:\n",
    "   - `df_engineered`: After feature engineering (new features created).\n",
    "   - `df_transformed`: After applying transformations (log, square root, etc.).\n",
    "   - `df_normalized`: If the data has been normalized.\n",
    "   - `df_standardized`: If the data has been standardized.\n",
    "\n",
    "4. **Encoding and Formatting**:\n",
    "   - `df_encoded`: After encoding categorical variables (one-hot, label encoding).\n",
    "   - `df_binned`: After binning continuous variables.\n",
    "   - `df_pivoted`: If data has been pivoted or reshaped.\n",
    "   - `df_aggregated`: After aggregation operations (group by, etc.).\n",
    "\n",
    "5. **Splitting**:\n",
    "   - `df_train`: Training set.\n",
    "   - `df_test`: Test set.\n",
    "   - `df_validate`: Validation set.\n",
    "\n",
    "6. **Modeling**:\n",
    "   - `df_predictions`: Contains model predictions.\n",
    "   - `df_residuals`: Residuals from model predictions.\n",
    "   - `df_analyzed`: DataFrames used for deeper analysis post-modeling.\n",
    "\n",
    "7. **Results and Export**:\n",
    "   - `df_results`: Final results or outputs.\n",
    "   - `df_export`: Data ready to be exported to a file or database.\n",
    "\n",
    "8. **Special Cases**:\n",
    "   - `df_merged`: After merging with another DataFrame.\n",
    "   - `df_joined`: After joining with another DataFrame.\n",
    "   - `df_sampled`: If a sample has been taken from the data.\n",
    "   - `df_segmented`: If the data has been segmented (e.g., by customer type).\n",
    "\n",
    "Each name corresponds to a common data processing or analysis task and makes it easier to track the purpose of each DataFrame in your workflow. Remember, these are just examples, and the actual names should align with the specific operations and logic of your project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b60c6a9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
